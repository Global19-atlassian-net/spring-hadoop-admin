<?xml version="1.0" encoding="UTF-8" standalone="no"?><beans:beans xmlns:beans="http://www.springframework.org/schema/beans" xmlns="http://www.springframework.org/schema/hadoop" xmlns:batch="http://www.springframework.org/schema/batch" xmlns:context="http://www.springframework.org/schema/context" xmlns:p="http://www.springframework.org/schema/p" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.springframework.org/schema/batch http://www.springframework.org/schema/batch/spring-batch-2.1.xsd   http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd   http://www.springframework.org/schema/hadoop http://www.springframework.org/schema/hadoop/spring-hadoop.xsd   http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-3.0.xsd">

    <context:property-placeholder ignore-resource-not-found="true" ignore-unresolvable="true" location="hadoop.properties"/>

 	
 	<configuration>	   
		fs.default.name=${hd.fs}
		mapred.job.tracker=${mapred.job.tracker}
	</configuration>
	
	<batch:job id="wordcount-withscript-job">
		<batch:step id="import" next="wordcount">
			<batch:tasklet ref="script-tasklet"/>
		</batch:step>
		<batch:step id="wordcount">
			<batch:tasklet ref="wordcount-tasklet"/>
		</batch:step>
	</batch:job>

	<tasklet id="wordcount-tasklet" job-ref="wc-job"/>

    <beans:bean class="org.springframework.data.hadoop.util.PathUtils" name="pathUtils" p:pathFormat="year/month/day/hour/minute/second" p:rootPath="/user/hadoop/output"/>

    <job id="wc-job" input-path="/user/hadoop/input" jar="${wordcount.jar.path}" mapper="org.apache.hadoop.examples.WordCount.TokenizerMapper" output-path="#{@pathUtils.getTimeBasedPathFromRoot()}" reducer="org.apache.hadoop.examples.WordCount.IntSumReducer" validate-paths="false"/>


	<script-tasklet id="script-tasklet">
  	   <script language="groovy">
  	   	// 'hack' default permissions to make Hadoop work on Windows
		if (System.getProperty("os.name").startsWith("Windows")) {
			// 0655 = -rwxr-xr-x
			 org.apache.hadoop.mapreduce.JobSubmissionFiles.JOB_DIR_PERMISSION.fromShort((short) 0655);
			 org.apache.hadoop.mapreduce.JobSubmissionFiles.JOB_FILE_PERMISSION.fromShort((short) 0655);
		}

  	   
		inputPath = "/user/hadoop/input"
		outputPath = "/user/hadoop/output"	
		if (fsh.test(inputPath)) {
		  fsh.rmr(inputPath)
		}
		if (fsh.test(outputPath)) {
		  fsh.rmr(outputPath)
		}

		// copy using the streams directly (to be portable across envs)
		inStream = cl.getResourceAsStream("data/nietzsche-chapter-1.txt")
		org.apache.hadoop.io.IOUtils.copyBytes(inStream, fs.create(inputPath), cfg)
	  </script>	
    </script-tasklet>


</beans:beans>